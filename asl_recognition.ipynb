{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Sign Language (ASL) Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFile  \n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "models_dir = 'saved_models'\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing data into training, validation, and testing sets\n",
    "\n",
    "Now that preprocessing the images is completed (see `data_preprocessing.ipynb` notebook), the full dataset will be split into training, validation, and testing sets. The testing set will be all the images from one subject to mirror the \"Spelling It Out\" paper's method so the benchmark model can be compared. The rest of the images will be randomly split; 80% of images for training, 20% of the images for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, n_categories=24):\n",
    "    data = load_files(path)\n",
    "    image_files = np.array(data['filenames'])\n",
    "    # Hot encode categories to matrix\n",
    "    image_targets = np_utils.to_categorical(np.array(data['target']), n_categories)\n",
    "    return image_files, image_targets\n",
    "\n",
    "def move_data_by_category(container_dir, regex_file_format='.*png'):\n",
    "    '''Move data into a directory based on category'''\n",
    "    # Still check if files are images\n",
    "    file_list = [x for x in os.listdir(container_dir) if re.search(regex_file_format, x)]\n",
    "    # Get numerical string (note that 1 digits are represented w/ 2 digits) \n",
    "    letters = {x.split('_')[0] for x in file_list}\n",
    "    \n",
    "    for letter in letters:\n",
    "        # Only images that match letter\n",
    "        images_with_letter = [filename for filename in file_list if filename.split('_')[0] == letter]\n",
    "        # Add images to sub directory\n",
    "        new_categ_path = os.path.join(container_dir, letter)\n",
    "        if not os.path.exists(new_categ_path):\n",
    "            os.makedirs(new_categ_path)\n",
    "        print(f'Created {new_categ_path} dir with {len(images_with_letter)} items')\n",
    "        for img_filename in images_with_letter:\n",
    "            path = os.path.join(container_dir, img_filename)\n",
    "            new_path = os.path.join(new_categ_path, img_filename)            \n",
    "            os.rename(path, new_path)\n",
    "    # TODO: Check if any files were skipped (improperly named?)\n",
    "        \n",
    "\n",
    "def get_testing_data(data_dir, subject_num='4'):\n",
    "    '''Get all data/images pertaining to one subject'''\n",
    "    # Only search in directory for images with that subject\n",
    "    file_list = [x for x in os.listdir(data_dir) if re.search(f'\\d+_{subject_num}_\\d*\\.png', x)]\n",
    "    \n",
    "    # Make a new testing data directory if doesn't exist\n",
    "    testing_dir = os.path.join(data_dir, 'test')\n",
    "    if not os.path.exists(testing_dir):\n",
    "        os.makedirs(testing_dir)\n",
    "        \n",
    "    # Move images of particular subject into testing directory\n",
    "    for image_filename in file_list:\n",
    "        # file is **_n_****.png where n is an integer representing a subject\n",
    "        _, subject, _ = image_filename.split('_')\n",
    "        # Move file into testing directory\n",
    "        path = os.path.join(data_dir, image_filename)\n",
    "        new_path = os.path.join(testing_dir, image_filename)\n",
    "        os.rename(path, new_path)\n",
    "        \n",
    "    # Move each image file's numerical str representing letters found in testing into own category directory\n",
    "    move_data_by_category(testing_dir)\n",
    "    \n",
    "    return load_dataset(testing_dir)\n",
    "\n",
    "\n",
    "def get_training_validation_data(data_dir, ratio=0.8):\n",
    "    '''Randomly split data into training and validation sets'''\n",
    "    # Only search in directory for images\n",
    "    file_list = [x for x in os.listdir(data_dir) if re.search('.*png', x)]\n",
    "    \n",
    "    # Make a new training & validation data directory if doesn't exist\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    valid_dir = os.path.join(data_dir, 'valid')\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(valid_dir):\n",
    "        os.makedirs(valid_dir)\n",
    "        \n",
    "    # Randomly split file list into training and vaidation sets\n",
    "    np.random.shuffle(file_list)\n",
    "    split_int = int(ratio * len(file_list))\n",
    "    train_list = file_list[:split_int]\n",
    "    valid_list = file_list[split_int:]\n",
    "    \n",
    "    # Move images of particular subject into testing directory\n",
    "    for filenames, new_dir in [(train_list, train_dir), (valid_list, valid_dir)]:\n",
    "        for image_filename in filenames:\n",
    "            # Move file into testing directory\n",
    "            path = os.path.join(data_dir, image_filename)\n",
    "            new_path = os.path.join(new_dir, image_filename)\n",
    "            os.rename(path, new_path)\n",
    "\n",
    "        # Move each image file's numerical str representing letters found in testing into own category directory\n",
    "        move_data_by_category(new_dir)\n",
    "    \n",
    "    return (load_dataset(train_dir), load_dataset(valid_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_targets = get_testing_data(data_dir)\n",
    "train, valid = get_training_validation_data(data_dir)\n",
    "# Separated data and its targets\n",
    "train_data, train_targets = train\n",
    "valid_data, valid_targets = valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8675309)\n",
    "%matplotlib inline\n",
    "\n",
    "# Display image previews below\n",
    "plt.figure(figsize=(20,55))\n",
    "columns = 8\n",
    "n = 1\n",
    "\n",
    "# Randomly choose images to display (with label)\n",
    "for image_path in np.random.choice(train_data, 24, replace=False):\n",
    "    img = Image.open(image_path)\n",
    "    plt.subplot(20, columns, n)\n",
    "    n+=1\n",
    "    plt.imshow(img)\n",
    "    letter = image_path.split('/')[-1][:2]\n",
    "    letter = chr(int(letter)+65)\n",
    "    plt.title(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data fro Keras (TensorFlow backend)\n",
    "def path_to_tensor(img_path):\n",
    "    # Loads image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224), grayscale=True)\n",
    "    # Convert PIL.Image.Image type to 3D tensor with shape (224, 224, 1)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 1) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10376/10376 [00:15<00:00, 676.35it/s]\n",
      "100%|██████████| 13898/13898 [00:19<00:00, 711.09it/s]\n"
     ]
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "valid_tensors = paths_to_tensor(valid_data).astype('float32')\n",
    "test_tensors = paths_to_tensor(test_data).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41500/41500 [01:02<00:00, 668.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data separately since this is usually large\n",
    "train_tensors = paths_to_tensor(train_data).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNN model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 224, 224, 16)      80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1560      \n",
      "=================================================================\n",
      "Total params: 11,976\n",
      "Trainable params: 11,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "#Convo 224, 224, 1\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(224, 224, 1)))\n",
    "#\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "#\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "#\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "#\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "#\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "#\n",
    "model.add(GlobalAveragePooling2D())\n",
    "#Dense; 24 for each handshape \n",
    "model.add(Dense(24, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41500 samples, validate on 10376 samples\n",
      "Epoch 1/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 2.4591 - acc: 0.2532Epoch 00001: val_loss improved from inf to 2.23496, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 159s 4ms/step - loss: 2.4591 - acc: 0.2532 - val_loss: 2.2350 - val_acc: 0.3183\n",
      "Epoch 2/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 2.1881 - acc: 0.3277Epoch 00002: val_loss improved from 2.23496 to 2.11627, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 2.1881 - acc: 0.3277 - val_loss: 2.1163 - val_acc: 0.3328\n",
      "Epoch 3/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.9669 - acc: 0.3845Epoch 00003: val_loss improved from 2.11627 to 1.77322, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 134s 3ms/step - loss: 1.9666 - acc: 0.3845 - val_loss: 1.7732 - val_acc: 0.4525\n",
      "Epoch 4/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.7827 - acc: 0.4372Epoch 00004: val_loss did not improve\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 1.7825 - acc: 0.4373 - val_loss: 1.8597 - val_acc: 0.4069\n",
      "Epoch 5/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.6169 - acc: 0.4895Epoch 00005: val_loss improved from 1.77322 to 1.43868, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 134s 3ms/step - loss: 1.6168 - acc: 0.4896 - val_loss: 1.4387 - val_acc: 0.5638\n",
      "Epoch 6/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.4773 - acc: 0.5338Epoch 00006: val_loss did not improve\n",
      "41500/41500 [==============================] - 134s 3ms/step - loss: 1.4771 - acc: 0.5339 - val_loss: 1.4971 - val_acc: 0.5092\n",
      "Epoch 7/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.3713 - acc: 0.5658Epoch 00007: val_loss improved from 1.43868 to 1.26232, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 1.3715 - acc: 0.5658 - val_loss: 1.2623 - val_acc: 0.6050\n",
      "Epoch 8/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.2734 - acc: 0.5955Epoch 00008: val_loss improved from 1.26232 to 1.23570, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 1.2733 - acc: 0.5955 - val_loss: 1.2357 - val_acc: 0.6054\n",
      "Epoch 9/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.1826 - acc: 0.6231Epoch 00009: val_loss improved from 1.23570 to 1.17843, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 1.1825 - acc: 0.6232 - val_loss: 1.1784 - val_acc: 0.6246\n",
      "Epoch 10/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.1059 - acc: 0.6470Epoch 00010: val_loss improved from 1.17843 to 1.04986, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 1.1057 - acc: 0.6471 - val_loss: 1.0499 - val_acc: 0.6717\n",
      "Epoch 11/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 1.0435 - acc: 0.6643Epoch 00011: val_loss improved from 1.04986 to 0.92161, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 1.0434 - acc: 0.6643 - val_loss: 0.9216 - val_acc: 0.6966\n",
      "Epoch 12/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.9854 - acc: 0.6845Epoch 00012: val_loss did not improve\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.9856 - acc: 0.6844 - val_loss: 1.0428 - val_acc: 0.6693\n",
      "Epoch 13/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.9352 - acc: 0.6996Epoch 00013: val_loss improved from 0.92161 to 0.80817, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.9352 - acc: 0.6996 - val_loss: 0.8082 - val_acc: 0.7457\n",
      "Epoch 14/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.8916 - acc: 0.7103Epoch 00014: val_loss did not improve\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.8914 - acc: 0.7104 - val_loss: 0.8840 - val_acc: 0.7118\n",
      "Epoch 15/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.7262Epoch 00015: val_loss improved from 0.80817 to 0.75392, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.8544 - acc: 0.7263 - val_loss: 0.7539 - val_acc: 0.7587\n",
      "Epoch 16/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.8186 - acc: 0.7377Epoch 00016: val_loss improved from 0.75392 to 0.72559, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.8184 - acc: 0.7377 - val_loss: 0.7256 - val_acc: 0.7696\n",
      "Epoch 17/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.7900 - acc: 0.7464Epoch 00017: val_loss did not improve\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.7899 - acc: 0.7464 - val_loss: 0.7807 - val_acc: 0.7496\n",
      "Epoch 18/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.7644 - acc: 0.7512Epoch 00018: val_loss did not improve\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.7645 - acc: 0.7511 - val_loss: 0.7841 - val_acc: 0.7421\n",
      "Epoch 19/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.7388 - acc: 0.7589Epoch 00019: val_loss did not improve\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.7390 - acc: 0.7589 - val_loss: 0.7791 - val_acc: 0.7422\n",
      "Epoch 20/20\n",
      "41480/41500 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.7667Epoch 00020: val_loss improved from 0.72559 to 0.66752, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "41500/41500 [==============================] - 135s 3ms/step - loss: 0.7134 - acc: 0.7667 - val_loss: 0.6675 - val_acc: 0.7893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97d1642198>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 20\n",
    "\n",
    "# Create a saved models directory\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=f'{models_dir}/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 43.0781%\n"
     ]
    }
   ],
   "source": [
    "handshape_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(handshape_predictions)==np.argmax(test_targets, axis=1))/len(handshape_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://alexisbcook.github.io/2017/using-transfer-learning-to-classify-images-with-keras/\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "## Extract Bottleneck Features for Train Set\n",
    "\n",
    "big_x_train = np.array([scipy.misc.imresize(train_data[i], (160, 160, 1)) \n",
    "                        for i in range(0, len(train_data))]).astype('float32')\n",
    "\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=True, input_shape=(160, 160, 1))\n",
    "print('model loaded')\n",
    "# Remove final layer\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n",
    "\n",
    "\n",
    "\n",
    "print('bottleneck features file not detected (train)')\n",
    "print('calculating now ...')\n",
    "resnet_train = preprocess_input(big_x_train)\n",
    "features = model.predict(resnet_train)\n",
    "features = np.squeeze(features)\n",
    "np.savez('resnet_train', features=features)\n",
    "print('bottleneck features saved (train)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract bottleneck features\n",
    "\n",
    "print('calculating now ...')\n",
    "# pre-process the test data\n",
    "big_x_valid = np.array([scipy.misc.imresize(valid_data[i], (139, 139, 3)) \n",
    "                   for i in range(0, len(valid_data))]).astype('float32')\n",
    "resnet_input_valid = preprocess_input(big_x_valid)\n",
    "# extract, process, and save bottleneck features (test)\n",
    "features_valid = model.predict(resnet_input_valid)\n",
    "features_valid = np.squeeze(features_valid)\n",
    "np.savez('inception_features_test', features_test=features_valid)\n",
    "print('bottleneck features saved (test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train network\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, GlobalAveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=100, kernel_size=2, input_shape=features.shape[1:]))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(24, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(features, train_targets, batch_size=50, epochs=16,\n",
    "           validation_data=(valid_tensors, valid_targets),, callbacks=[checkpointer],\n",
    "          verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
